{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"autoencoder.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"3yHydVLI-YzQ","colab_type":"code","colab":{}},"source":["\n","\n","#                              AUTOENCODER\n","\n","\n","#***********************************IMPORTS*************************************\n","\n","\n","# Install TensorFlow\n","try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","\n","import tensorflow as tf\n","import numpy as np\n","from google.colab import drive\n","import sys\n","import os\n","\n","\n","\n","drive.mount('/content/gdrive', force_remount=True)\n","project_path = \"/content/gdrive/My Drive/shared/Colab Notebooks/tesi/models\"           #PATH NEED TO BE CHANGED ACCORDING TO THE LOCATION OF THE PROJECT\n","data_path = \"/content/gdrive/My Drive/shared/Colab Notebooks/tesi/data/\"\n","weights_path = project_path + '/weights/'\n","sys.path.append(project_path)\n","\n","from evaluation_utilities import *\n","from data_utilities import *\n","from net_utilities import *\n","\n","\n","#*******************************************************************************"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qn5bGME8rNf1","colab_type":"code","colab":{}},"source":["#************************************PARAMS*************************************\n","\n","\n","n_classes = None\n","batch_size = 128\n","random_seed = 1995\n","n_epoch = 1999\n","input_size = (28, 28, 1)\n","\n","\n","np.random.seed(seed=random_seed)\n","tf.random.set_seed(seed=random_seed)\n","\n","#*******************************************************************************"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BuSKyjMSfH6k","colab_type":"code","colab":{}},"source":["#******************************DATA PROCESSING**********************************\n","\n","\n","print(\"loading data...\")\n","\n","\n","# **** load draws ****\n","(X_draws, y_string_draws) = load_data(data_path + \"/draws-28.pickle\", size=input_size[0], _3d=False, invert=False, randomize=False, rand_seed=random_seed)\n","\n","# **** load icons ****\n","(X_icons, y_string_icons) = load_data(data_path + \"/icons-28.pickle\", size=input_size[0], _3d=False, invert=False, randomize=False, rand_seed=random_seed)\n","\n","\n","# **** check datasets classes ****\n","X_draws, X_icons, y_string_draws, y_string_icons = check_dataset_classes(X_draws, X_icons, y_string_draws, y_string_icons)\n","\n","\n","# **** preprocess  draws ****\n","x_train_draws, x_valid_draws, x_test_draws, y_train_draws, y_valid_draws, y_test_draws = split_dataset(X_draws, y_string_draws, _validation_size=0.2, _test_size=0.1, _random_seed=random_seed, stratify=True)\n","y_train_draws, y_valid_draws, y_test_draws = labels_preprocessing(y_train_draws, y_valid_draws, y_test_draws)\n","x_train_draws, y_train_draws = shuffle_with_same_indexes(x_train_draws, y_train_draws, seed=random_seed)\n","x_valid_draws, y_valid_draws = shuffle_with_same_indexes(x_valid_draws, y_valid_draws, seed=random_seed)\n","x_test_draws, y_test_draws = shuffle_with_same_indexes(x_test_draws, y_test_draws, seed=random_seed)\n","x_train_draws, x_valid_draws, x_test_draws = data_preprocessing(x_train_draws), data_preprocessing(x_valid_draws), data_preprocessing(x_test_draws)\n","\n","\n","\n","# **** preprocess  icons ****\n","x_train_icons, x_valid_icons, x_test_icons, y_train_icons, y_valid_icons, y_test_icons = split_dataset(X_icons, y_string_icons, _validation_size=0.2, _test_size=0.1, _random_seed=random_seed, stratify=True)\n","y_train_icons, y_valid_icons, y_test_icons = labels_preprocessing(y_train_icons, y_valid_icons, y_test_icons)\n","x_train_icons, y_train_icons = shuffle_with_same_indexes(x_train_icons, y_train_icons, seed=random_seed)\n","x_valid_icons, y_valid_icons = shuffle_with_same_indexes(x_valid_icons, y_valid_icons, seed=random_seed)\n","x_test_icons, y_test_icons = shuffle_with_same_indexes(x_test_icons, y_test_icons, seed=random_seed)\n","x_train_icons, x_valid_icons, x_test_icons = data_preprocessing(x_train_icons), data_preprocessing(x_valid_icons), data_preprocessing(x_test_icons)\n","\n","\n","\n","# **** check datasets ****\n","X_draws, X_icons, y_string_draws, y_string_icons = check_dataset_classes(X_draws, X_icons, y_string_draws, y_string_icons)\n","\n","print(\"data loaded\")\n","\n","#*******************************************************************************"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AndpFaVtfOOf","colab_type":"code","colab":{}},"source":["\n","# **** preprocess icons ****\n","for _i, i in enumerate(x_train_icons): x_train_icons[_i] = contour_img(i)\n","for _i, i in enumerate(x_valid_icons): x_valid_icons[_i] = contour_img(i)\n","for _i, i in enumerate(x_test_icons): x_test_icons[_i] = contour_img(i)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HEwGkHsufOTs","colab_type":"code","colab":{}},"source":["# **** merge datasets ****\n","\n","x_train = np.concatenate((x_train_draws, x_train_icons), axis=0)\n","x_valid = np.concatenate((x_valid_draws, x_valid_icons), axis=0)\n","x_test = np.concatenate((x_test_draws, x_test_icons), axis=0)\n","\n","y_train = np.concatenate((y_train_draws, y_train_icons), axis=0)\n","y_valid = np.concatenate((y_valid_draws, y_valid_icons), axis=0)\n","y_test = np.concatenate((y_test_draws, y_test_icons), axis=0)\n","\n","x_train, y_train = shuffle_with_same_indexes(x_train, y_train, seed=random_seed)\n","x_valid, y_valid = shuffle_with_same_indexes(x_valid, y_valid, seed=random_seed)\n","x_test, y_test = shuffle_with_same_indexes(x_test, y_test, seed=random_seed)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RB9kxHB90qwd","colab_type":"code","colab":{}},"source":["is_data_already_prepro = False if np.max(x_train[0]) > 1 else True\n","print(\"is data preprocessed? \", is_data_already_prepro)\n","\n","custom_aug = CustomAug({\n","      'rescale': not is_data_already_prepro,        # if rescale == True, the alg assumes the data is in format 0-255\n","      'pad' : True,                           \n","      'horizontal_flip' : True,             \n","      'erosion' : True,                       \n","      'half_aug' : True,\n","    }\n",")\n","\n","\n","\n","class AutoencoderGenerator(tf.keras.utils.Sequence):\n","    'Generates data for Keras'\n","    def __init__(self, x, batch_size=128, shuffle=True):\n","        self.x = x\n","        self.batch_size = batch_size\n","        self.shuffle = True\n","        self.dim = (28,28,1)\n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        'Denotes the number of batches per epoch'\n","        return int(np.floor(len(self.x) / self.batch_size))\n","\n","    def __getitem__(self, index):\n","        'Generate one batch of data'\n","        # Generate indexes of the batch\n","        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n","\n","        # Generate data\n","        X, y = self.__data_generation(indexes)\n","\n","        return X, y\n","\n","    def on_epoch_end(self):\n","        'Updates indexes after each epoch'\n","        self.indexes = np.arange(len(self.x))\n","\n","        if self.shuffle == True:\n","            np.random.shuffle(self.x)\n","\n","    def __data_generation(self, indexes):\n","        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n","        # Initialization\n","        X = np.empty((self.batch_size, *self.dim))\n"," \n","        # Generate data\n","        for i, index in enumerate(indexes):\n","            # Store sample\n","            X[i,] = custom_aug.custom_preprocessing(self.x[index])\n","\n","        return X, X\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8NNTQMoVfObV","colab_type":"code","colab":{}},"source":["\n","train_gen = AutoencoderGenerator(x_train)\n","valid_gen = AutoencoderGenerator(x_valid)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KK1OVqKMhrOu","colab_type":"code","colab":{}},"source":["\n","\n","class ReversedSimpleEmbeddingNet(tf.keras.Model):\n","\n","  def __init__(self):\n","    self.filter_size = 24\n","    super(ReversedSimpleEmbeddingNet, self).__init__()\n","\n","    self.l1_dense   = tf.keras.layers.Dense(768, activation='relu')\n","    self.l1_resh    = tf.keras.layers.Reshape((4, 4, self.filter_size*2))\n","    self.l1_conv    = tf.keras.layers.Conv2DTranspose(self.filter_size*2,kernel_size=5,strides=2,padding='same',activation='relu')\n","    self.l1_batch   = tf.keras.layers.BatchNormalization()\n","    self.l2_conv    = tf.keras.layers.Conv2DTranspose(self.filter_size*2,kernel_size=3,activation='relu')\n","    self.l2_batch   = tf.keras.layers.BatchNormalization()\n","    self.l3_conv    = tf.keras.layers.Conv2DTranspose(self.filter_size*2,kernel_size=3,activation='relu')\n","    self.l3_drop    = tf.keras.layers.Dropout(0.4)\n","    self.l3_batch   = tf.keras.layers.BatchNormalization()\n","    self.l4_conv    = tf.keras.layers.Conv2DTranspose(self.filter_size,kernel_size=5,strides=2,padding='same',activation='relu')\n","    self.l4_batch   = tf.keras.layers.BatchNormalization()\n","    self.l5_conv    = tf.keras.layers.Conv2DTranspose(self.filter_size,kernel_size=3,activation='relu')\n","    self.l5_batch   = tf.keras.layers.BatchNormalization()\n","    self.l6_conv    = tf.keras.layers.Conv2DTranspose(self.filter_size,kernel_size=3,activation='relu',input_shape=(28,28,1))\n","    self.l7_conv    = tf.keras.layers.Conv2DTranspose(1,kernel_size=3,activation='sigmoid', padding=\"same\", input_shape=(28,28,1))\n","\n","  def call(self, x):\n","    x = self.l1_dense(x)           \n","    x = self.l1_resh(x)            \n","    x = self.l1_conv(x)            \n","    x = self.l1_batch(x)           \n","    x = self.l2_conv(x)            \n","    x = self.l2_batch(x)           \n","    x = self.l3_conv(x)            \n","    x = self.l3_drop(x)            \n","    x = self.l3_batch(x)         \n","    x = self.l4_conv(x)            \n","    x = self.l4_batch(x)         \n","    x = self.l5_conv(x)           \n","    x = self.l5_batch(x)          \n","    x = self.l6_conv(x)                  \n","    x = self.l7_conv(x)                            \n","    return x\n","\n","\n","class Autoencoder(tf.keras.Model):\n","\n","  def __init__(self):\n","    super(Autoencoder, self).__init__()\n","\n","    self.encoder = SimpleEmbeddingNet()\n","    self.decoder = ReversedSimpleEmbeddingNet()  \n","\n","  def call(self, x):\n","\n","    x = self.encoder.call(x) \n","    x = self.decoder.call(x)\n","    return x\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lktOZa0xfOZm","colab_type":"code","colab":{}},"source":["\n","#*********************************************************************************************************************************************\n","net_callbacks = [\n","\tPlotLosses(),\n","  tf.keras.callbacks.ModelCheckpoint(weights_path + 'autoencoder_' + \"{epoch:02d}\"  + '.h5',  monitor='val_loss', verbose=1, period=20, save_best_only=True, mode='min'),\n","\t#tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, min_delta=1, verbose=1, mode='auto', restore_best_weights=True)\n","]\n","net_callbacks.append(AutoencoderViz())\n","\n","print(\"net_callbacks var created\")\n","#*********************************************************************************************************************************************\n","\n","lr =  0.008   #0.0001\n","\n","autoencoder = Autoencoder()\n","adam = tf.keras.optimizers.Adam(learning_rate=lr)\n","\n","autoencoder.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n","autoencoder.call(tf.zeros((1, 28,28,1)))\n","\n","\n","history = autoencoder.fit(\n","    train_gen,\n","    epochs=n_epoch,\n","    verbose=1, \n","    callbacks=net_callbacks,  \n","    validation_data=valid_gen,\n","    initial_epoch=0\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MTWsueGOPqGe","colab_type":"code","colab":{}},"source":["\n","''' test set must be preprocessed like the training and valid generator data '''\n","if np.max(x_test) > 1:\n","    x_test = data_preprocessing(x_test)\n","\n","\n","autoencoder.load_weights(weights_path + '/autoencoder_20.h5')    \n","print(autoencoder.evaluate(x_test, x_test))\n","embedding_net = autoencoder.encoder\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pWPsU8jNA2qB","colab_type":"code","colab":{}},"source":["X_icons_eval = load_data(path=data_path + \"/icons_eval.pickle\", size=input_size[0], invert=False, _3d=False, randomize=False, rand_seed=random_seed)\n","print(len(list(X_icons_eval)))\n","\n","# delete white images\n","X_icons_eval = np.asarray([i for i in X_icons_eval if np.min(i) != np.max(i)])\n","\n","X_icons_eval_edges = data_preprocessing(X_icons_eval)\n","\n","for _i, i in enumerate(X_icons_eval_edges): X_icons_eval_edges[_i] = contour_img(i)\n","\n","for i, im in enumerate(X_icons_eval_edges):\n","  if i < 10:\n","    show_img(im)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CxOZ2CiRA-HT","colab_type":"code","colab":{}},"source":["manual_eval = RankImages(X_icons_eval_edges, embedding_net, _n=15, _show_im=False, _show_dist=False)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JNZHW-AbBA2Z","colab_type":"code","colab":{}},"source":["\n","images = [cv2.imread(os.path.join(data_path, 'targets', im_path), 0) for im_path in os.listdir(os.path.join(data_path, 'targets')) if im_path.endswith('.jpg')]\n","images = [cv2.resize(i, (input_size[0], input_size[1])) for i in images]\n","\n","for image in images:\n","  target_im = image\n","\n","  target_im = np.expand_dims(data_preprocessing(target_im), axis=-1)\n","\n","  #it should be commented...\n","  target_im_edges = target_im\n","  #target_im_edges = contour_img(target_im)\n","\n","  res = manual_eval.get_n_most_similar_images(target_im_edges, _returnType='indexes')\n","  manual_eval.format_result(target_im, [X_icons_eval_edges[r] for r in res])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CI2dAc84BZZz","colab_type":"code","colab":{}},"source":["print(\"creating vector space...\")\n","x_feat_test_icons = np.array([embedding_net.predict(f[np.newaxis, ...]) for f in x_test_icons])[:, 0, :]\n","x_feat_test_draws = np.array([embedding_net.predict(f[np.newaxis, ...]) for f in x_test_draws])[:, 0, :]\n","\n","print(\"vector space created\")\n","\n","knn = KNN(x_feat_test_icons, y_test_icons, k=10)\n","\n","y_test_icons_true = y_test_icons                                                # np.array_equal(y_test_icons,y_test_icons) is True\n","y_test_draws_pred = knn.get_labels(x_feat_test_draws)\n","\n","get_score(y_test_icons_true, y_test_draws_pred)\n","\n"],"execution_count":0,"outputs":[]}]}
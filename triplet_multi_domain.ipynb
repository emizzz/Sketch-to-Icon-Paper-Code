{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"triplet_multi_domain.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"3yHydVLI-YzQ","colab_type":"code","colab":{}},"source":["#                              TRIPET NET\n","\n","\n","\n","# Install TensorFlow\n","try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","\n","import tensorflow as tf\n","import numpy as np\n","from google.colab import drive\n","import sys\n","import os\n","from tensorflow.keras import backend as K\n","\n","\n","drive.mount('/content/gdrive', force_remount=True)\n","project_path = \"/content/gdrive/My Drive/shared/Colab Notebooks/tesi/models\"           #PATH NEED TO BE CHANGED ACCORDING TO THE LOCATION OF THE PROJECT\n","data_path = \"/content/gdrive/My Drive/shared/Colab Notebooks/tesi/data/\"\n","weights_path = project_path + '/weights/'\n","sys.path.append(project_path)\n","\n","from evaluation_utilities import *\n","from data_utilities import *\n","from net_utilities import *\n","from generators_utilities import *\n","\n","\n","#*******************************************************************************"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qn5bGME8rNf1","colab_type":"code","colab":{}},"source":["#************************************PARAMS*************************************\n","\n","\n","n_classes = 169\n","batch_size = 128  # 32\n","random_seed = 1995\n","n_epoch = 1999\n","input_size = (28, 28, 1)\n","\n","np.random.seed(seed=random_seed)\n","tf.random.set_seed(seed=random_seed)\n","\n","#*******************************************************************************"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Sn18icxXQPv","colab_type":"code","colab":{}},"source":["\n","def _pairwise_distances(embeddings, squared=False):\n","    \"\"\"Compute the 2D matrix of distances between all the embeddings.\n","    Args:\n","        embeddings: tensor of shape (batch_size, embed_dim)\n","        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n","                 If false, output is the pairwise euclidean distance matrix.\n","    Returns:\n","        pairwise_distances: tensor of shape (batch_size, batch_size)\n","    \"\"\"\n","    # Get the dot product between all embeddings\n","    # shape (batch_size, batch_size)\n","    dot_product = tf.linalg.matmul(embeddings, tf.transpose(embeddings))\n","\n","    # Get squared L2 norm for each embedding. We can just take the diagonal of `dot_product`.\n","    # This also provides more numerical stability (the diagonal of the result will be exactly 0).\n","    # shape (batch_size,)\n","    square_norm = tf.linalg.diag_part(dot_product)\n","\n","    # Compute the pairwise distance matrix as we have:\n","    # ||a - b||^2 = ||a||^2  - 2 <a, b> + ||b||^2\n","    # shape (batch_size, batch_size)\n","    distances = tf.expand_dims(square_norm, 1) - 2.0 * dot_product + tf.expand_dims(square_norm, 0)\n","\n","    # Because of computation errors, some distances might be negative so we put everything >= 0.0\n","    distances = tf.math.maximum(distances, 0.0)\n","\n","    if not squared:\n","        # Because the gradient of sqrt is infinite when distances == 0.0 (ex: on the diagonal)\n","        # we need to add a small epsilon where distances == 0.0\n","        mask = tf.compat.v1.to_float(tf.equal(distances, 0.0))\n","        distances = distances + mask * 1e-16\n","\n","        distances = tf.math.sqrt(distances)\n","\n","        # Correct the epsilon added: set the distances on the mask to be exactly 0.0\n","        distances = distances * (1.0 - mask)\n","\n","    return distances\n","\n","\n","def _get_anchor_negative_triplet_mask(labels):\n","    \"\"\"Return a 2D mask where mask[a, n] is True iff a and n have distinct labels.\n","    Args:\n","        labels: tf.int32 `Tensor` with shape [batch_size]\n","    Returns:\n","        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n","    \"\"\"\n","    # Check if labels[i] != labels[k]\n","    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n","    labels_equal = tf.math.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n","\n","    mask = tf.math.logical_not(labels_equal)\n","\n","    return mask\n","\n","\n","def _get_anchor_positive_triplet_mask(labels):\n","    \"\"\"Return a 2D mask where mask[a, p] is True iff a and p are distinct and have same label.\n","    Args:\n","        labels: tf.int32 `Tensor` with shape [batch_size]\n","    Returns:\n","        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n","    \"\"\"\n","    # Check that i and j are distinct\n","    indices_equal = tf.dtypes.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n","    indices_not_equal = tf.math.logical_not(indices_equal)\n","\n","    # Check if labels[i] == labels[j]\n","    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n","    labels_equal = tf.math.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n","    \n","    # Combine the two masks\n","    mask = tf.math.logical_and(indices_not_equal, labels_equal)\n","\n","    return mask\n","\n","def _get_anchor_positive_triplet_mask_multi_domain(labels):\n","\n","    # *************************multi domain version*****************************\n","    mask = tf.math.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n","    # **************************************************************************\n","\n","    return mask\n","\n","    # *************************multi domain version*****************************\n","def multidomain_pairwise_dist(A, B):\n","    \"\"\"\n","    Computes pairwise distances between each elements of A and each elements of B.\n","    Args:\n","      A,    [m,d] matrix\n","      B,    [n,d] matrix\n","    Returns:\n","      D,    [m,n] matrix of pairwise distances\n","    \"\"\"\n","    #with tf.variable_scope('pairwise_dist'):\n","    # squared norms of each row in A and B\n","    na = tf.reduce_sum(tf.square(A), 1)\n","    nb = tf.reduce_sum(tf.square(B), 1)\n","\n","    # na as a row and nb as a co\"lumn vectors\n","    na = tf.reshape(na, [-1, 1])\n","    nb = tf.reshape(nb, [1, -1])\n","\n","    # return pairwise euclidean difference matrix\n","    D = tf.sqrt(tf.maximum(na - 2 * tf.matmul(A, B, False, True) + nb, 0.0))\n","    return D\n","    # **************************************************************************\n","\n","\n","'''\n","    - INPUT:\n","        - y_pred = [emb1_d1, emb2_d1, emb3_d1, ..., emb1_d2, emb2_d2, emb3_d2, ...]\n","        - y_true = [label1_d1, label2_d1, label3_d1, ..., label1_d2, label2_d2, label3_d3, ...] (labels must be the same in the 2 domains)\n","\n","'''\n","def batch_hard_triplet_loss_multi_domain(y_true, y_pred, margin=1, squared=False):\n","\n","    #*************************keras need this***********************************\n","    labels = tf.squeeze(y_true, axis=-1)\n","    #***************************************************************************\n","  \n","    # *************************multi domain version*****************************\n","    #labels = labels[:labels.shape[0] // 2]                                      # labels must be the same in the 2 domains\n","    _batch_size = y_pred.shape[1] // 2\n","\n","    # embedding = (batch, feat)\n","    embedding_d = y_pred[:, :_batch_size]\n","    embedding_i = y_pred[:, _batch_size:]\n","    \n","    pairwise_dist_same = multidomain_pairwise_dist(embedding_i, embedding_i)\n","    pairwise_dist_diff = multidomain_pairwise_dist(embedding_i, embedding_d)\n","\n","    pairwise_dist = tf.linalg.set_diag(\n","        pairwise_dist_diff,\n","        tf.linalg.diag_part(                                                    # all zeros\n","            pairwise_dist_same,\n","        )\n","    )\n","\n","    \n","    # **************************************************************************\n","    # *************************single domain version****************************\n","    #pairwise_dist = _pairwise_distances(y_pred, squared=squared)\n","    # **************************************************************************\n","\n","    # For each anchor, get the hardest positive\n","    # First, we need to get a mask for every valid positive (they should have same label)\n","    \n","    # *************************multi domain version*****************************\n","    mask_anchor_positive = _get_anchor_positive_triplet_mask_multi_domain(labels)\n","    # **************************************************************************\n","    # *************************single domain version*****************************\n","    #mask_anchor_positive = _get_anchor_positive_triplet_mask(labels)\n","    # **************************************************************************\n","\n","    mask_anchor_positive = tf.compat.v1.to_float(mask_anchor_positive)\n","\n","    # We put to 0 any element where (a, p) is not valid (valid if a != p and label(a) == label(p))\n","    anchor_positive_dist = tf.math.multiply(mask_anchor_positive, pairwise_dist)\n","\n","    # shape (batch_size, 1)\n","    hardest_positive_dist = tf.math.reduce_max(anchor_positive_dist, axis=1, keepdims=True)\n","    tf.summary.scalar(\"hardest_positive_dist\", tf.math.reduce_mean(hardest_positive_dist))\n","\n","    # For each anchor, get the hardest negative\n","    # First, we need to get a mask for every valid negative (they should have different labels)\n","    mask_anchor_negative = _get_anchor_negative_triplet_mask(labels)\n","    mask_anchor_negative = tf.compat.v1.to_float(mask_anchor_negative)\n","\n","    # We add the maximum value in each row to the invalid negatives (label(a) == label(n))\n","    max_anchor_negative_dist = tf.math.reduce_max(pairwise_dist, axis=1, keepdims=True)\n","    anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (1.0 - mask_anchor_negative)\n","\n","    # shape (batch_size,)\n","    hardest_negative_dist = tf.math.reduce_min(anchor_negative_dist, axis=1, keepdims=True)\n","    tf.summary.scalar(\"hardest_negative_dist\", tf.math.reduce_mean(hardest_negative_dist))\n","\n","    # Combine biggest d(a, p) and smallest d(a, n) into final triplet loss\n","    triplet_loss = tf.math.maximum(hardest_positive_dist - hardest_negative_dist + margin, 0.0)\n","\n","    # Get final mean triplet loss\n","    triplet_loss = tf.math.reduce_mean(triplet_loss)\n","\n","    return triplet_loss\n","\n","#*********************************************************************************************************************************************"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"l6X6rfIc1oOM","colab_type":"code","colab":{}},"source":["#******************************DATA PROCESSING**********************************\n","\n","\n","print(\"loading data...\")\n","\n","\n","# **** load draws ****\n","(X_draws, y_string_draws) = load_data(data_path + \"/draws-28.pickle\", size=input_size[0], _3d=False, invert=False, randomize=False, rand_seed=random_seed)\n","\n","# **** load icons ****\n","(X_icons, y_string_icons) = load_data(data_path + \"/icons-28.pickle\", size=input_size[0], _3d=False, invert=False, randomize=False, rand_seed=random_seed)\n","\n","\n","# **** check datasets classes ****\n","X_draws, X_icons, y_string_draws, y_string_icons = check_dataset_classes(X_draws, X_icons, y_string_draws, y_string_icons)\n","\n","\n","# **** preprocess  draws ****\n","x_train_draws, x_valid_draws, x_test_draws, y_train_draws, y_valid_draws, y_test_draws = split_dataset(X_draws, y_string_draws, _validation_size=0.2, _test_size=0.1, _random_seed=random_seed)\n","y_train_draws, y_valid_draws, y_test_draws = labels_preprocessing(y_train_draws, y_valid_draws, y_test_draws)\n","x_train_draws, y_train_draws = shuffle_with_same_indexes(x_train_draws, y_train_draws, seed=random_seed)\n","x_valid_draws, y_valid_draws = shuffle_with_same_indexes(x_valid_draws, y_valid_draws, seed=random_seed)\n","x_test_draws, y_test_draws = shuffle_with_same_indexes(x_test_draws, y_test_draws, seed=random_seed)\n","x_train_draws, x_valid_draws, x_test_draws = data_preprocessing(x_train_draws), data_preprocessing(x_valid_draws), data_preprocessing(x_test_draws)\n","\n","\n","\n","# **** preprocess  icons ****\n","x_train_icons, x_valid_icons, x_test_icons, y_train_icons, y_valid_icons, y_test_icons = split_dataset(X_icons, y_string_icons, _validation_size=0.2, _test_size=0.1, _random_seed=random_seed)\n","y_train_icons, y_valid_icons, y_test_icons = labels_preprocessing(y_train_icons, y_valid_icons, y_test_icons)\n","x_train_icons, y_train_icons = shuffle_with_same_indexes(x_train_icons, y_train_icons, seed=random_seed)\n","x_valid_icons, y_valid_icons = shuffle_with_same_indexes(x_valid_icons, y_valid_icons, seed=random_seed)\n","x_test_icons, y_test_icons = shuffle_with_same_indexes(x_test_icons, y_test_icons, seed=random_seed)\n","x_train_icons, x_valid_icons, x_test_icons = data_preprocessing(x_train_icons), data_preprocessing(x_valid_icons), data_preprocessing(x_test_icons)\n","\n","\n","\n","# **** check datasets ****\n","X_draws, X_icons, y_string_draws, y_string_icons = check_dataset_classes(X_draws, X_icons, y_string_draws, y_string_icons)\n","assert len(set(y_train_draws)) == n_classes, print(\"wrong class number\")\n","assert len(set(y_train_icons)) == n_classes, print(\"wrong class number\")\n","\n","print(\"data loaded\")\n","\n","#*******************************************************************************"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CWLwDkSV4gu9","colab_type":"code","colab":{}},"source":["#***********************************NETWORK*************************************\n","\n","\n","class SingleNet(tf.keras.Model):\n","\n","  def __init__(self):\n","    super(SingleNet, self).__init__()\n","    self.filter_size = 24\n","\n","    self.l1_conv    = tf.keras.layers.Conv2D(self.filter_size,kernel_size=3,activation='relu',input_shape=(28,28,1), name=\"l1_conv\")\n","    self.l1_batch   = tf.keras.layers.BatchNormalization(name=\"l1_batch\")\n","    self.l2_conv    = tf.keras.layers.Conv2D(self.filter_size,kernel_size=3,activation='relu', name=\"l2_conv\")\n","    self.l2_batch   = tf.keras.layers.BatchNormalization(name=\"l2_batch\")\n","    self.l3_conv    = tf.keras.layers.Conv2D(self.filter_size,kernel_size=5,strides=2,padding='same',activation='relu', name=\"l3_conv\")\n","    self.l3_batch   = tf.keras.layers.BatchNormalization(name=\"l3_batch\")\n","    self.l3_dropout = tf.keras.layers.Dropout(0.4, name=\"l3_dropout\")\n","    self.l4_conv    = tf.keras.layers.Conv2D(self.filter_size*2,kernel_size=3,activation='relu', name=\"l4_conv\")\n","    self.l4_batch   = tf.keras.layers.BatchNormalization(name=\"l4_batch\")\n","\n","  \n","  def call(self, x):\n","    x = self.l1_conv(x)\n","    x = self.l1_batch(x)\n","    x = self.l2_conv(x)\n","    x = self.l2_batch(x)\n","    x = self.l3_conv(x)\n","    x = self.l3_batch(x)\n","    x = self.l3_dropout(x)\n","    x = self.l4_conv(x)\n","    x = self.l4_batch(x)\n","\n","    return x\n","\n","\n","class SharedNet(tf.keras.Model):\n","\n","  def __init__(self, n_classes=None):\n","    super(SharedNet, self).__init__()\n","\n","    self.n_classes = n_classes\n","    self.filter_size = 24\n","\n","    self.l5_conv    = tf.keras.layers.Conv2D(self.filter_size*2,kernel_size=3,activation='relu', name=\"l5_conv\")\n","    self.l5_batch   = tf.keras.layers.BatchNormalization(name=\"l5_batch\")\n","    self.l6_conv    = tf.keras.layers.Conv2D(self.filter_size*2,kernel_size=5,strides=2,padding='same',activation='relu', name=\"l6_conv\")\n","    self.l6_batch   = tf.keras.layers.BatchNormalization(name=\"l6_batch\")\n","    self.l6_dropout = tf.keras.layers.Dropout(0.4, name=\"l6_dropout\")\n","    self.l7_flatten = tf.keras.layers.Flatten(name=\"l7_flatten\")\n","    self.l7_dense   = tf.keras.layers.Dense(128, activation='relu', name=\"l7_dense\")\n","\n","    if self.n_classes:\n","      self.l8_batch   = tf.keras.layers.BatchNormalization(name=\"classification1\")\n","      self.l8_dropout = tf.keras.layers.Dropout(0.4, name=\"classification2\")\n","      self.l8_dense   = tf.keras.layers.Dense(n_classes, activation='softmax', name=\"classification3\")\n","  \n","  def call(self, x):\n","    x = self.l5_conv(x)\n","    x = self.l5_batch(x)\n","    x = self.l6_conv(x)\n","    x = self.l6_batch(x)\n","    x = self.l6_dropout(x)\n","    x = self.l7_flatten(x)\n","    x = self.l7_dense(x)\n","\n","    if self.n_classes:\n","      x = self.l8_batch(x)\n","      x = self.l8_dropout(x)\n","      x = self.l8_dense(x)\n","    \n","    return x\n","\n","class BranchNet(tf.keras.Model):\n","\n","  def __init__(self, shared_net=None, n_classes=None):\n","    super(BranchNet, self).__init__()\n","\n","    self.single_net = SingleNet()\n","\n","    # if a sharedNet instance is passed in args, then this net uses the reference\n","    # of the passed net, otherwise it creates a new BranchNet instance\n","    if shared_net is None:   \n","      self.shared_net = SharedNet(n_classes) if n_classes else SharedNet()   \n","    else:\n","      if n_classes: print(\"n_classes parameter ignored\")\n","      self.shared_net = shared_net\n","\n","\n","    self.single_net._name = \"single_net\"\n","    self.shared_net._name = \"shared_net\"\n","      \n","\n","  def call(self, image):\n","    image = self.single_net(image)\n","    image = self.shared_net(image)\n","    return image\n","\n","\n","\n","\n","class TripletNet(tf.keras.Model):\n","\n","  def __init__(self):\n","    super(TripletNet, self).__init__()\n","\n","    self.shared_net = SharedNet()\n","\n","    self.draws_net = BranchNet(self.shared_net)\n","    self.icons_net = BranchNet(self.shared_net)\n","\n","\n","  def call(self, img_input):\n","\n","    draw, icon = img_input\n","\n","    draw = self.draws_net.call(draw)\n","    icon = self.icons_net.call(icon)\n","\n","    return tf.keras.layers.concatenate([draw, icon])\n","\n","\n","lr = 0.0001\n","\n","drawsIconsTriplet = TripletNet()\n","drawsIconsTriplet.compile(tf.keras.optimizers.Adam(lr), loss=batch_hard_triplet_loss_multi_domain, run_eagerly=True) \n","  \n","drawsIconsTriplet.build([(1, 28, 28,1), (1, 28, 28,1)])\n","_ = drawsIconsTriplet.call(\n","    [tf.zeros((1, 28, 28, 1)), tf.zeros((1, 28, 28, 1))]\n",")\n","#*******************************************************************************\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wqy4uTZEmSY5","colab_type":"code","colab":{}},"source":["tl_net = SimpleNet(n_classes=345)\n","tl_net.compile(tf.keras.optimizers.Adam(0.0001), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=['acc'])\n","tl_net.build((1, 28, 28,1))\n","\n","\n","tl_net.load_weights(weights_path + \"/cnn_quickdraw15_filter24.h5\")\n","tl_net_embedding = tl_net.embedding_net\n","\n","load_nested_net_weights(tl_net_embedding, drawsIconsTriplet.draws_net)\n","load_nested_net_weights(tl_net_embedding, drawsIconsTriplet.icons_net)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LhAILtNxGU30","colab_type":"code","outputId":"ebc62ed8-0b14-443e-a159-a4fb786a3fc0","executionInfo":{"status":"ok","timestamp":1583058812457,"user_tz":-60,"elapsed":11404,"user":{"displayName":"emilio ver","photoUrl":"","userId":"11983886235773277334"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["\n","# **** create generators ****\n","\n","is_data_already_prepro = False if np.max(x_train_draws[0]) > 1 else True\n","print(\"is data preprocessed? \", is_data_already_prepro)\n","\n","custom_aug = CustomAug({\n","      'rescale': not is_data_already_prepro,        # if rescale == True, the alg assumes the data is in format 0-255\n","      'pad' : True,                           \n","      'horizontal_flip' : True,             \n","      'erosion' : True,                       \n","      'half_aug' : True,\n","    }\n",")\n","\n","augment_draws = False\n","if augment_draws:\n","  triplet_training_generator = TripletDataGenerator(x_train_draws, y_train_draws, x_train_icons, y_train_icons, batch_size=batch_size, shuffle=True, transf_anchor_func=custom_aug.custom_preprocessing, transf_image_func=None) \n","  triplet_validation_generator = TripletDataGenerator(x_valid_draws, y_valid_draws, x_valid_icons, y_valid_icons, batch_size=batch_size, shuffle=True, transf_anchor_func=custom_aug.custom_preprocessing, transf_image_func=None)\n","else:\n","  triplet_training_generator = TripletDataGenerator(x_train_draws, y_train_draws, x_train_icons, y_train_icons, batch_size=batch_size, shuffle=True) \n","  triplet_validation_generator = TripletDataGenerator(x_valid_draws, y_valid_draws, x_valid_icons, y_valid_icons, batch_size=batch_size, shuffle=True)\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["is data preprocessed?  True\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZXOggDxtbWL4","colab_type":"code","colab":{}},"source":["#**********************************TRAINING*************************************\n","\n","net_callbacks = [\n","\tPlotLosses(),\n","  tf.keras.callbacks.ModelCheckpoint(weights_path + 'triplet_' + \"{epoch:02d}\"  + '.h5',  monitor='val_loss', verbose=1, period=5, save_best_only=True, mode='min'),\n","\t#tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, min_delta=1, verbose=1, mode='auto', restore_best_weights=True)\n","]\n","print(\"net_callbacks var created\")\n","\n","\n","drawsIconsTriplet.fit(\n","    triplet_training_generator,\n","    epochs=n_epoch,\n","    verbose=2, \n","    callbacks=net_callbacks,\n","    validation_data=triplet_validation_generator,\n",")\n","\n","\n","\n","\n","#*******************************************************************************"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JI_pdDoke37H","colab_type":"code","colab":{}},"source":["drawsIconsTriplet.load_weights(weights_path + 'no_aug_triplet_20.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gR0L_Zz6mN1S","colab_type":"code","colab":{}},"source":["X_icons_eval = load_data(path=data_path + \"/icons_eval.pickle\", size=input_size[0], invert=False, _3d=False, randomize=False, rand_seed=random_seed)\n","\n","# delete white images\n","X_icons_eval = np.asarray([i for i in X_icons_eval if np.min(i) != np.max(i)])\n","\n","X_icons_eval_edges = data_preprocessing(X_icons_eval)\n","\n","#for _i, i in enumerate(X_icons_eval_edges): X_icons_eval_edges[_i] = contour_img(i)\n","\n","for i, im in enumerate(X_icons_eval_edges):\n","  if i < 10:\n","    print(np.min(im), np.max(im))\n","    show_img(im)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YMlITLajfANU","colab_type":"code","colab":{}},"source":["print(\"triplet network trained:\")\n","\n","manual_eval = RankImages(X_icons_eval_edges, drawsIconsTriplet.icons_net, drawsIconsTriplet.draws_net, _n=15, _show_im=False, _show_dist=False)\n","#target_im = paint_brush_img(w=input_size[0], h=input_size[1], line_width=30, preprocessed=True, show=False)\n","images = [cv2.imread(os.path.join(data_path, 'targets', im_path), 0) for im_path in os.listdir(os.path.join(data_path, 'targets')) if im_path.endswith('.jpg')]\n","images = [cv2.resize(i, (input_size[0], input_size[1])) for i in images]\n","\n","for image in images:\n","  target_im = image\n","  target_im = np.expand_dims(data_preprocessing(target_im), axis=-1)\n","  #target_im = contour_img(target_im)\n","  res = manual_eval.get_n_most_similar_images(target_im, _returnType='indexes')\n","  manual_eval.format_result(target_im, [X_icons_eval[r] for r in res])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AZAyFCSDzSoR","colab_type":"code","colab":{}},"source":["print(\"creating vector space...\")\n","x_feat_test_icons = np.array([drawsIconsTriplet.icons_net.predict(f[np.newaxis, ...]) for f in x_test_icons])[:, 0, :]\n","x_feat_test_draws = np.array([drawsIconsTriplet.draws_net.predict(f[np.newaxis, ...]) for f in x_test_draws])[:, 0, :]\n","\n","print(\"vector space created\")\n","\n","knn = KNN(x_feat_test_icons, y_test_icons, k=10)\n","\n","y_test_icons_true = y_test_icons                                                # np.array_equal(y_test_icons,y_test_icons) is True\n","y_test_draws_pred = knn.get_labels(x_feat_test_draws)\n","\n","get_score(y_test_icons_true, y_test_draws_pred)\n","\n","\n","\n"],"execution_count":0,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transfer_learning.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"3yHydVLI-YzQ","colab_type":"code","colab":{}},"source":["\n","\n","#                              RESNET vs MOVILENET vs SIMPLE CNN\n","\n","\n","#***********************************IMPORTS*************************************\n","\n","# Install TensorFlow\n","try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","\n","import tensorflow as tf\n","import numpy as np\n","from google.colab import drive\n","import sys\n","import os\n","\n","\n","drive.mount('/content/gdrive', force_remount=True)\n","project_path = \"/content/gdrive/My Drive/shared/Colab Notebooks/tesi/models\"           #PATH NEED TO BE CHANGED ACCORDING TO THE LOCATION OF THE PROJECT\n","data_path = \"/content/gdrive/My Drive/shared/Colab Notebooks/tesi/data/\"\n","weights_path = project_path + '/weights/'\n","sys.path.append(project_path)\n","\n","from evaluation_utilities import *\n","from data_utilities import *\n","from net_utilities import *\n","\n","\n","#*******************************************************************************"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qn5bGME8rNf1","colab_type":"code","colab":{}},"source":["#************************************PARAMS*************************************\n","\n","\n","n_classes = 345\n","batch_size = 128\n","random_seed = 1995\n","n_epoch = 1999\n","model =  'mobilenet' #['cnn', 'mobilenet', 'resnet']\n","dataset = 'quickdraw' # synth, quickdraw\n","\n","if model == 'cnn':\n","  input_size = (28, 28, 1)\n","else:\n","  input_size = (32, 32, 3)\n","\n","\n","\n","np.random.seed(seed=random_seed)\n","tf.random.set_seed(seed=random_seed)\n","\n","#*******************************************************************************"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dSYvRp-pRpCf","colab_type":"code","colab":{}},"source":["#******************************DATA PROCESSING**********************************\n","\n","\n","print(\"loading data...\")\n","\n","_X = []\n","_y = []\n","\n","\n","# it loads some npy files\n","if dataset == \"quickdraw\":\n","  for cat in os.listdir(os.path.join(data_path, 'quickdraw_npy')):\n","    temp_data = np.load(os.path.join(data_path, 'quickdraw_npy', cat))\n","    _X.extend(temp_data)\n","    for it in range(len(temp_data)):\n","      _y.append(cat)\n","    print(cat, \"loaded\")\n","\n","# it loads some pickle files\n","elif dataset == \"synth\":\n","  import pickle\n","  for cat in os.listdir(os.path.join(data_path, 'synth_04_pick')):\n","    with open(os.path.join(data_path, 'synth_04_pick', cat), 'rb') as pickle_file:\n","      temp_data_x, temp_data_y = pickle.load(pickle_file)\n","    _X.extend(temp_data_x)\n","    _y.extend(temp_data_y)    \n","    print(cat, \"loaded\")\n","\n","\n","_invert = False if dataset == \"synth\" else True\n","if model == \"cnn\":  \n","  X, y_string = load_data(data=[_X, _y], size=input_size[0], _3d=False, invert=_invert, randomize=False, rand_seed=random_seed)\n","#the resnet/mobilenet must loads the 28x28 (onebyte) images because of ram limit (generator then preprocess the data)\n","else:\n","  X, y_string = load_data(data=[_X, _y], size=input_size[0], _3d=True, invert=_invert, randomize=False, rand_seed=random_seed)\n","del _X, _y\n","\n","#*******************************************************************************\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BV0BzZBRaNkc","colab_type":"code","outputId":"cb943537-3d43-4b90-fb3c-7ab2cd118298","executionInfo":{"status":"ok","timestamp":1583657838710,"user_tz":-60,"elapsed":10143,"user":{"displayName":"emilio ver","photoUrl":"","userId":"11983886235773277334"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["\n","x_train, x_valid, x_test, y_train, y_valid, y_test = split_dataset(X, y_string, _validation_size=0.2, _test_size=0.1, _random_seed=random_seed, stratify=True)\n","\n","y_train, y_valid, y_test = labels_preprocessing(y_train, y_valid, y_test)\n","\n","x_train, y_train = shuffle_with_same_indexes(x_train, y_train, seed=random_seed)\n","x_valid, y_valid = shuffle_with_same_indexes(x_valid, y_valid, seed=random_seed)\n","x_test, y_test = shuffle_with_same_indexes(x_test, y_test, seed=random_seed)\n","\n","del X, y_string\n","\n","assert len(set(y_train)) == n_classes, print(\"wrong class number\")\n","\n","print(\"data loaded\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["created: x_train:  (993600, 32, 32, 3)\n","created: x_valid:  (248400, 32, 32, 3)\n","created: x_test:  (138000, 32, 32, 3)\n","created: y_string_train:  (993600,)\n","created: y_string_valid:  (248400,)\n","created: y_string_test:  (138000,)\n","created: num classes:  345 345 345\n","data loaded\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OyUZ4ygIq58B","colab_type":"code","outputId":"19686ddd-012a-4c7b-b2c2-7e06fbc53ac5","executionInfo":{"status":"ok","timestamp":1583657852574,"user_tz":-60,"elapsed":12073,"user":{"displayName":"emilio ver","photoUrl":"","userId":"11983886235773277334"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["\n","is_data_already_prepro = False if np.max(x_train[0]) > 1 else True\n","print(\"is data preprocessed? \", is_data_already_prepro)\n","\n","custom_aug = CustomAug({\n","      'rescale': not is_data_already_prepro,        # if rescale == True, the alg assumes the data is in format 0-255\n","      'pad' : True,                           \n","      'horizontal_flip' : True,             \n","      'erosion' : True,                       \n","      'half_aug' : True,\n","    }\n",")\n","\n","\n","datagen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=custom_aug.custom_preprocessing)\n","train_gen = datagen.flow(x_train, y_train, batch_size=batch_size)\n","valid_gen = datagen.flow(x_valid, y_valid, batch_size=batch_size)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["is data preprocessed?  False\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Qu_gGvNVq1F3","colab_type":"code","outputId":"039a6f68-f17d-433b-ca30-11a70835c78e","executionInfo":{"status":"ok","timestamp":1583657886062,"user_tz":-60,"elapsed":1472,"user":{"displayName":"emilio ver","photoUrl":"","userId":"11983886235773277334"}},"colab":{"base_uri":"https://localhost:8080/","height":752}},"source":["for i, im in enumerate(x_train):\n","  if i < 15:\n","    mm = custom_aug.custom_preprocessing(im)\n","    #print(np.asarray(mm))\n","    print(np.min(np.asarray(mm)), np.max(np.asarray(mm)), np.asarray(mm).shape)\n","    show_img(mm)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.007843138 1.0 (32, 32, 3)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABOklEQVR4nGP8z4AfMBGQHykKWBDM\nv7++fXz37du3b780tbhZMBR8Pvzoyf0n75l+ff7+S9jE0leeDSLOCAvqG773eeWlFRSE+FgubX37\nTjkkWRQi8R8K3voLOF54//f/////vt8+FcNmuOjT//////+HK/gzlUfvHozzY5svl4B14XNkBf+3\nCwlMhXP+fVlqK8435d+/f3A3MDwNOyMeJf/h8at3FW4MDAwM9/e26G5gQDLhz/l+WW4BSUUdpup/\n///////3vanpq9+/EeHAbKCVdP4NF8/ruHu/2RgYGJi4JW98EEQOKAY2NnsGBobHko+/sTEwMPy6\nxfjlByOKAgjglbo2S+rnpzevHlz9xIlVgdnlXkFR5v8MPDYqEkyIkESAx+d+MvFzcHJyc3LzMGBT\ngAoGQ4oaEgoAr9etAXCmyE0AAAAASUVORK5CYII=\n","text/plain":["<PIL.Image.Image image mode=L size=32x32 at 0x7F067606A550>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["0.015686275 1.0 (32, 32, 3)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAAiklEQVR4nGP8z4AfMBGQH1UwrBSw\n4JL4zcDM9JORDZuCx/dfMv35cP/ahR+ivW5MCAW//zHfuvv88bPXL788/crIwPBflP3VejsuuILf\nvbMkhZnei/E+ucKqqqooy/f3H7+qFCcDIyxN/nt5Q0hcgoHh0mM2IUEBQUYGhv+MDAwMCAVke3NU\nAZEKACUkKkwkWzXGAAAAAElFTkSuQmCC\n","text/plain":["<PIL.Image.Image image mode=L size=32x32 at 0x7F067606A8D0>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["0.0 1.0 (32, 32, 3)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAByElEQVR4nGP8z4AfMBGQJ6DgGz4F\nTydYWvJsIWwFC4bI/0f3GO7fu3/z/H92hv98DAz/UcG9BkUGBgYGDi3frlMCAsH//6MqWOPAyKyk\nISUlc+X///9pHBz30BUIcFgycElKSoopvf5/kbm6+j+6AnNHlng2FgaGTs7i/zl8nz+jKXj9mqWB\noZeVlZf3nb/qX6no//9RFfzz9eW4IRdhzTR9egmr20GGDf//////n2A4IJnQysAw538U07xYTk6W\nnPc5vN/RrLjLnpDw//8jDQaevLx7/29xpvxHUxAi8Pr1/++T72kV/fr1/6+tyEs0BccYe/7/nzaJ\nQT+BMT///wSG1VBxRmiC+Wfx9ho7QwYDx9nNi2UZ9PU8V6E5ciLDOoR//7kIv4BxoAoe8vogvLNg\nAcMyOIfYcPDlfwLX80dV1RJhHETBXIZZCKElDAzbEDzG/wwMJxkcXDYxMjAwMPy5fOLUyceqDGcZ\n4TYw/md4bsLAd4L/yfGTJxnOfWOQ+/BpA4M/A7KCiO0MGy+sOMnAa8pgbm4uUfB2EQMjsiNPMXJx\nMTFaTbr89z82wMLwjFGdISJcHpcvCYYD43+G75wEFOAHBK0AAIjTnVrL/TzmAAAAAElFTkSuQmCC\n","text/plain":["<PIL.Image.Image image mode=L size=32x32 at 0x7F067606AAC8>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["0.015686275 1.0 (32, 32, 3)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAAq0lEQVR4nGP8z4AfMBGQH1WAV8Hv\nCx9+QZksWBUc2v6Tq40Zhwmff/x4dYoh79AvTBP+TNn89atABMeK24bHSpWeqmAoYDp+1N2w96HI\no4/ueif+/cV05IN/Vzey/qibzLTxLM/iKFVMK37+fKUcdmxZjOiPV0wHOWGi/5FAs963LTNuuBp7\n9LyBizEip6j7nezv8k3e7gtFEkNR8P/3YxZ5NF8zjqbJQaMAAG7hVfJ1y3zoAAAAAElFTkSuQmCC\n","text/plain":["<PIL.Image.Image image mode=L size=32x32 at 0x7F067606AAC8>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["0.0 1.0 (32, 32, 3)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACBElEQVR4nLWTX0hTcRTHv/e669bU\n3C0taQXLJUWUFDkoe4haRa4IaqCXXvLFWy9BRE+jzB5s2GPRU0T/iwVRBJWxQBBFVi9Sy/bgaLO2\nyka2nBXrzm8P2y43h+lL36dzzu9zDt/fgSMQ/5Y4x/vcgAkA8HowWq/IhvK3noGJzUfgmscEkD86\nOkRUwKIkEiwoUy/t2F8rSddJgWi/jRO/U5WpYC36rEOxpH0XAp0XTpbfvIjhuBVMmny+5vIGM5yy\nbCsDrACkliWN69eGQrhFE55oB3HubKf22HsA4kb3Kvn9VZyZHH84pmSALzChChZsuhRxxnJeKACw\nQsXzZfYtUaQBGeBnk6qObABqjuZyRZP02I8tcPn94keCPA2onz7cjYc1TQd6V+473i/LO8n57GG6\nq0sq29ogIhCgQa+WOxwJUiAAvLsTtIcjY1habBu896a/LojVAPQORyv50/tgUukjeQhNvjRJg4fv\nsWZgYqSG4SEApyR398Kih7xC6DU68DoLgT4hgjVG73ui0XygA1OoNgJ2pGYAs0kHFiMFgFfi+fSF\n1KgvKq8BPCU5VbEtn7pdhboOZBe1ZLNMit0kybfC+ZkAL4sez/gvf5oktbbKryUAb5jN1e33M2Q6\nvRc9LAU4OqrWwbJ7u81mvqYXhb9Pb/rlo2dV63C4qfSbs0n4/8f7B4HgYGJxRzWcAAAAAElFTkSu\nQmCC\n","text/plain":["<PIL.Image.Image image mode=L size=32x32 at 0x7F067606AA90>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["0.0 1.0 (32, 32, 3)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABeklEQVR4nGP8z4AfMBGQHxQKWHDK\n/GdgYEQo+M/ACGH8/sfMwsDw/+vD+09+iHgJQRX8vnT6r7Y5J8P/fy8vvefV/Pzy8fVbj1//VNTg\nZ4YoeD353k+Zvw7MP77PW/1JtPjNrRsX3v9h/M94U5uTgYWBgeH/l6vMvx/eM+N5e+3MbaGbs779\n5A9Tubr55fvVLhAFf+/w5Apt2/hf58z6B7of310r8RPk/Pj6L9PfpzeEWVgYGBiYNIqt+CUWrFv1\n7L7qH66Zky/m/f5+48pvpr//WBkZWBgYGJgUFRkZ1KsuHdnE+ky33vz28mlvn9y59ecfU6ge1JGM\nDAwMjHw26t/uMUdZMD77XMXEzScs857ZiBs1oLh4GT5eUHt7iklb2Ipn2y0dTw20kGSX4nw9d+mP\n/2rWin5fLt6X1eRGU8Aiq8z6h/G/VLiJkJhwnJOuFjsDAwMjcor6s/ft17/ssmZ8DAz//zAxMzCg\nK0CJFSjAUIAOBkOSGyEKAPxZjxyf2h7WAAAAAElFTkSuQmCC\n","text/plain":["<PIL.Image.Image image mode=L size=32x32 at 0x7F067606A710>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["0.0 1.0 (32, 32, 3)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABPElEQVR4nOWSzyuDcRzHX8/2bItE\nazkhWm1obk8UJSI5KSk9/wVNHOZC7aYc5OSycrHNQQ5z4SBnSn5NPcOi1Epb5DDU9nF4NM922FyU\n8j59vu/P59v39X33UYTqstXo/6+B9RZ1xuqeRo9KtYhkMg2D06TFVD6iacDkg3lEREIhZ/qEhGkY\nfjQtcrvW1PEkIiI1GRQBH4HdbPPYCLzlk/uerSGA84HeA1RA5Aw2RZY9AO6uxccvlhjxuMmw5HDk\nRKQg5Sq26bqIqLAzjBuw5e4pWh6vGz0EULm66CN6ZxipbAWe6mRhtlWR+VVAbff7/F4clv7z3Cuy\noSMJgsHj9woAKdys2MPhn+UgE3u4xvuVb+8DI3mdV6YiNJpBXRLbTlku2fEGunu0zlKSAC+WH9bj\nKo+6qv7OTv7qwCdiUanmIV2VqAAAAABJRU5ErkJggg==\n","text/plain":["<PIL.Image.Image image mode=L size=32x32 at 0x7F067606A550>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["0.0 1.0 (32, 32, 3)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABLElEQVR4nN2SMUsCARiGn5O7Szrt\njIrDCGtTQ6KWoMWWfoA5NLW0t+YYHA1Bf6Ch39CQbUUQ0hQKDVJJIGQQBeqdEGSkfQ1pRFYKbb7T\nB+/7fnw8fIrwtzxd/D4JqO3hNVd4AMAfjjL+PfBytJ92vtTMcMin+zYxQeVmJ0/+yVpJTE/gAarX\nlxSuii4lHzYgByZsnDakQ4vxuIh0PRKRbWh29kXWLUtE1N+rqvTEoRdQRciNcH9xV6ZcKVeaAQW/\nOjignAVagSmYBxgaY9QbmrXqb9RwxZE6t5MgIsmkEtw6rkqptKqAtpBKHTrP1WJa12O1njiISCaq\nJ5YjmmbY2RN7yTA+3T1RPr66Fn6MzMVYCwI0yJ43DdNrMDyj0cJW33V/ointDf8A1R+Bd7AopjVU\nz9S+AAAAAElFTkSuQmCC\n","text/plain":["<PIL.Image.Image image mode=L size=32x32 at 0x7F067606A860>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["0.0 1.0 (32, 32, 3)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABNElEQVR4nGP8z4AfMBGQH1UABSwo\nvP/Hflmxo6n4jwyuCfBt+I8KUK3Y/PHPsud4THgspRPMF/UGxQRkBT/m8i47z8fX/weXgvXeEo9+\nzDe0fInLDTuumkuyxwbdPo8jHH7eeabLzMBs8uMmciJCCoc3L37drGJj+8i5I0oEiy/+XSySYpAW\nFeHjZOI5gM2RzyyZeQWrH54KMxHgrsGm4DwPS+v+N19yGBkYGJYiKYC74enPf+u+PjixSkL0DrMm\nNke+/Bv5aMtHRq8skWQeOWwK7ohWSH54ya7N8ZlVhgObAsa/byU5BR/P/f3niSEzNgXiH1ed+f3x\n1rmPbH+V2LApMFRe8vfvn39alo/YbZHDnxEWrP8/vmJgZOfiZv7LyM6ITQHD/7+MDIyMyHIMDAzU\nSNUAFSj/n5ku4T0AAAAASUVORK5CYII=\n","text/plain":["<PIL.Image.Image image mode=L size=32x32 at 0x7F067606A6A0>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["0.043137256 1.0 (32, 32, 3)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABHklEQVR4nGP8z4AfMBGQH4kKfv/B\nooAFwXxXxaghwcsmKiHIhl3BUS6LeZe+//vLP90Pq4JfR13dA35//XXr0iorEWwKGHk+MLKxcTPI\nOCS/QlKAcCRryJY3DAwMDAz//yG7AckXiv8uMTAw/H+68wMXdkcyCRSK87KzfHjFwY6kgBGRYN5G\nFvH92PeCU+22XAlWK25qOYouv8h4cbroja9Yrfgg+C/KbCPT/xfJzHf1sJnwg/3tGztGBkZhdvaH\nWK0Qes7D85SBgeHnf41bCJchWaF0/Z/fntB/t4+oMwszYlMgwPKgfF4el4SSQ2cCVit4vSZ9CY4z\n/H0qRlUeazgwvO+6xcTOysMuGSqHXQHDv59MzMyMDCiAcShkPQC+DVetV7cbPgAAAABJRU5ErkJg\ngg==\n","text/plain":["<PIL.Image.Image image mode=L size=32x32 at 0x7F067606A860>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["0.0 1.0 (32, 32, 3)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABeUlEQVR4nM3Tv0sCYRgH8Od9z0s8\nz7ATysA0LUEEKYKICIogaopoCGpsaO6faHUqiJrahBYlaGvQ1iiChiLExNCTukNP8cL07mnITM3O\noaXv9vB+Xt4fDw9BMA7tsd4bmNoq/b1OKUOZ3wDencqCIAgjPpupG8CX6EEJAGzD0wuBSVsn0KTn\nwzO6yChKIV3K+7QZS2NbI3pqZ84+e5xTxdvI7lKATJ1IOiJiE7yFedvGjYqIqBUfzrcdQ3uvOiI2\nj3iKcKFNJ1SrsgpmdC7X4vvy6jzTvIOeTbr8VwkxAzrRKoQgz4pHSb+rCbCsVDK8UE3XgGWZNb2W\nr2jqRWILyFcv6vePqZykCP1sMSqEXW6UEpfXscD3M02hECICAInHJlZYSgYsDo+btn0UIQAAIBfG\nzASA9XrW2c5eAACAyI1/SkqhazdF3ttSdQEyN9oDWDyGQC3zfYZAVK1gCLK1QWOQI15jwAWDrSX5\nMRdqwWo3BB35++D8B/ABx/6XphdLsnIAAAAASUVORK5CYII=\n","text/plain":["<PIL.Image.Image image mode=L size=32x32 at 0x7F067606A668>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["0.0 1.0 (32, 32, 3)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABuUlEQVR4nN2TP0hbYRTFz3uGR2Ji\nLBgXNx2MEZshBJQkg1CcHAwFGywIdXBQFwUHHVzbJ4U21EE6lSDSokMstERMUVQqYkIRMaKgiVSQ\nqJH6J8ZEEk+H5sUmBXWUnu2798d37z2XKxC3S7wj/yAAVd7rOBLeN9vUeTEqykxaLAAATUtLIhel\nAlx9qIHZ/M63dfKtXxBe/gMc16OhORgkmeladblKjwqA6GPt1EdMT5P8Du/2ttRXADzVL3LjNUmy\nV5cgn1fmgPuMuXEIr+yAyQQMO40XLzRA2XlclxvzXNJo9GckGQjg2Z9/6/BGKaGCrs2DyyZjxcDu\nOKTaH5aFr6dYh/HGqB2tVvvKXpz1tKZTACCKRwyMZ6fwYHOT7G4abARKAam9emSk6sm1p6QxC7gR\ni5Fk+mBtTYYtRM7MYMAKqz8YXEoRDBV1dJCMqBtkeWUqQ9Lp1CsNvL/bB5Ay0JNk+q1DFDFG8qdK\n1V+L1onPfv9ciiCvh4ZEa5hkNDoWJS9dkrTP0S95u/AZHn3aiSWTsfnR7nK43TfbVta9Z1dKGtqW\n+beE7OGkZ09/xaE21ZUX9Cj8F5f1G7I6SNz59yFXAAAAAElFTkSuQmCC\n","text/plain":["<PIL.Image.Image image mode=L size=32x32 at 0x7F067606A710>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["0.0 1.0 (32, 32, 3)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABdElEQVR4nJWTv0tCURzFz3v+wB8N\nSZEkEpZEBhUJbWYNgf0F7Y5OtbW0CYFTi+21tDTmYNDWIFIRVCRCQqQWNqSiFQ/s9W14vvuu90nZ\nmc499/Pu+94DVyL8LqtuyhetFhAMOgAA8oSsb5Cm79neD3e6ObETZG/eC/XhqQMASJRZ/scI7BeU\nk7eJKYMD3TKANq2FguaaTf9SxwzUrcmk5tJp3LDYmMETyWY112hYFljMDRms1TSjKI4+RQEvvq6x\nQFUtfYBnvas5KHeLA/dgAMVSIKC5WMyzay6qPj1ereqLFHKmHtadl0aTn/5VEbjFPnFK41Eo6lTe\n4EdbQ04Y8io0xgMh970AuL56LieN1P/bw3CzN/c0BMD3Vub3qTrKrKaac4u/ZhFHYlEJ9ysHHKIk\nAiXnZD6vL97DU6aq6XrGZkupRETt9or9zAxQOx5HeK+inEej9hOWDv4uiIjoOCIBcLkyRiQJr7uS\n+ZhfxpARiIBJP11BiOd8aivVAAAAAElFTkSuQmCC\n","text/plain":["<PIL.Image.Image image mode=L size=32x32 at 0x7F067606A6A0>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["0.0 1.0 (32, 32, 3)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAABvklEQVR4nGP8z4AfsKALvDlzV5DB\nUxAh8B8FvC7gZGBgYGB1Pg0TQVXwSZct89S78+cb+FjuQ4WYCDgBxQ3/GaJu7bNmOL2I4Y3MI7jj\nkSzYt49hxv936UyCgqoMytjcEB8v+vOlNFvV16//Y0ywKdDUjPwfxnXh/////399ggkiu+EBA4vy\nPY2vDAwMDKysWMLhLCcnY4KXHEM5is8RCs7yy8s7uf//mMO4fj2SAoLhADfhs5Tqy5elEXdYd1sq\nKmKzooXp3LFjDJd++2+oZ2fHYsXvPoldLCwML1k2+H/h5MQS1KfeMTEKMDAsPZwvdF0DW1x8YPhn\nMp+BaYF6X+uRWGyO3M8wOZ+N7eCz/1qGbHfvIrkBZoIK0+dP1QwnDhfznShRQjaA+HDw5k988ULd\nQp7B/w/2oH6oysDAwMBoyHAERR4Rm3KNUa0M7J48CqetsVvx/zDDhg3///+30HmJYgIjPO39NPzJ\ncJGHYUsEVwGDmT229HCKhSXszf//t52YmRlmYjry////fX2sfA0Pv01iYnL5jBFZhMPh///////f\njmFiYGAICvqBzZEQ8GjH53dNDMwIAXQFGAAADV7JP3zJ43QAAAAASUVORK5CYII=\n","text/plain":["<PIL.Image.Image image mode=L size=32x32 at 0x7F067606A860>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["0.0 1.0 (32, 32, 3)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAB/klEQVR4nN2TO2hTYRSAvz+5NzWP\nm5rYPKygjaEJ1VaDqBirSBFxsSIONSgGxOJUBycXUQQfIKiDoEKhIg6pBSkOIoqDQ0WbYhR8pNGq\nNFAxQb2xSdUkkt/hNh21o3iWA+d858k5QvJnMf3F/08Ayqwu6rquo+tfdV3X+Vxr8rq79qMCMt8V\nCnnqHHaP362qgC1AS8uolEIOHIyRESHXQmuFL89SFfOqTax0T57oiR/A/wTkKTE0hMOId2w9fm9a\nSiml7GuU/f28kgrghMvmt4pHIybmmmv/ll8MVRRMuCBuWAMbZt0z5SDvqqDOZ0y7tIHFfi6Y7+Pk\nXRgbSZHKmC4w0QAWFDQsUBnYB0muwHS0FmRNPDFoeR+sAwJFOb98UfI6beAc9zQCb+4gKnWghKtz\ndCNEGEY+jgoge3NnwlEFFQUHRaIvxh/UWtsxk+kc3DPDrYvKsYT208jgpEhsb2a30XS448i1R5RW\n3JC4ssaYGqVSt3a0agDi0rpCb2/y6bKrrLda+TGfPTTv0Jg8ezh6ejtQy5a3ZV7Tk61xaKkXci4h\nb+8yckvbAtRCGZrChEOhtrAYYfPDLQrdYwW+lwsf7k/oSNTA6o4l+Jr9XoEP8iiY1hrFzlAmnX6Z\nfj78C8Dk87shN3dyAA1EIkAtT24qN5X7+InWFsR/8Vm/AQF5rAxVSRp3AAAAAElFTkSuQmCC\n","text/plain":["<PIL.Image.Image image mode=L size=32x32 at 0x7F067606A898>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"rk0TaQMQdIKx","colab_type":"code","colab":{}},"source":["class SimpleEmbeddingNet(tf.keras.Model):\n","\n","  def __init__(self):\n","    self.filter_size = 24\n","    self.first_kernel_size = 3\n","    self.embedding_size = 256\n","\n","    super(SimpleEmbeddingNet, self).__init__()\n","\n","    self.l1_conv    = tf.keras.layers.Conv2D(self.filter_size,kernel_size=self.first_kernel_size,activation='relu',input_shape=(28,28,1))\n","    self.l1_batch   = tf.keras.layers.BatchNormalization()\n","    self.l2_conv    = tf.keras.layers.Conv2D(self.filter_size,kernel_size=3,activation='relu')\n","    self.l2_batch   = tf.keras.layers.BatchNormalization()\n","    self.l3_conv    = tf.keras.layers.Conv2D(self.filter_size,kernel_size=5,strides=2,padding='same',activation='relu')\n","    self.l3_batch   = tf.keras.layers.BatchNormalization()\n","    self.l3_dropout = tf.keras.layers.Dropout(0.4)\n","    self.l4_conv    = tf.keras.layers.Conv2D(self.filter_size*2,kernel_size=3,activation='relu')\n","    self.l4_batch   = tf.keras.layers.BatchNormalization()\n","    self.l5_conv    = tf.keras.layers.Conv2D(self.filter_size*2,kernel_size=3,activation='relu')\n","    self.l5_batch   = tf.keras.layers.BatchNormalization()\n","    self.l6_conv    = tf.keras.layers.Conv2D(self.filter_size*2,kernel_size=5,strides=2,padding='same',activation='relu')\n","    self.l6_batch   = tf.keras.layers.BatchNormalization()\n","    self.l6_dropout = tf.keras.layers.Dropout(0.4)\n","    self.l7_flatten = tf.keras.layers.Flatten()\n","    self.l7_dense   = tf.keras.layers.Dense(self.embedding_size, activation='relu')\n","\n","  \n","  def call(self, x):\n","    x = self.l1_conv(x)\n","    x = self.l1_batch(x)\n","    x = self.l2_conv(x)\n","    x = self.l2_batch(x)\n","    x = self.l3_conv(x)\n","    x = self.l3_batch(x)\n","    x = self.l3_dropout(x)\n","    x = self.l4_conv(x)\n","    x = self.l4_batch(x)\n","    x = self.l5_conv(x)\n","    x = self.l5_batch(x)\n","    x = self.l6_conv(x)\n","    x = self.l6_batch(x)\n","    x = self.l6_dropout(x)\n","    x = self.l7_flatten(x)\n","    x = self.l7_dense(x)   \n","\n","    return x\n","\n","\n","class SimpleNet(tf.keras.Model):\n","\n","  def __init__(self, n_classes=10):\n","    super(SimpleNet, self).__init__()\n","\n","    self.embedding_net = SimpleEmbeddingNet()\n","\n","    self.l7_batch   = tf.keras.layers.BatchNormalization()\n","    self.l7_dropout = tf.keras.layers.Dropout(0.4)\n","    self.l8_dense   = tf.keras.layers.Dense(n_classes, activation='softmax')\n","\n","  def call(self, x):\n","    \n","    x = self.embedding_net.call(x) \n","\n","    x = self.l7_batch(x)\n","    x = self.l7_dropout(x)\n","    x = self.l8_dense(x)\n","\n","    return x\n","\n","\n","\n","\n","class ResNet(tf.keras.Model):\n","\n","  def __init__(self, n_classes=10):\n","    super(ResNet, self).__init__()\n","\n","    self.resnet = tf.keras.applications.ResNet50(input_tensor=tf.keras.layers.Input(shape=(32, 32, 3)), input_shape=input_size, include_top = False, pooling = 'avg', weights=None)\n","\n","    self.lastL1   = tf.keras.layers.BatchNormalization()\n","    self.lastL2   = tf.keras.layers.Dropout(0.4)\n","    self.lastL3   = tf.keras.layers.Dense(n_classes, activation='softmax')\n","\n","  def call(self, x):\n","\n","    x = self.resnet(x)\n","    x = self.lastL1(x)\n","    x = self.lastL2(x)\n","    x = self.lastL3(x)\n","\n","    return x\n","\n","\n","\n","class MobileNet(tf.keras.Model):\n","\n","  def __init__(self, n_classes=10):\n","    super(MobileNet, self).__init__()\n","\n","    self.mobilenet = tf.keras.applications.mobilenet.MobileNet(\n","        input_tensor=tf.keras.layers.Input(shape=(32, 32, 3)),\n","        input_shape=(input_size), \n","        alpha=1.0, \n","        include_top=False, \n","        weights=None, \n","        pooling='avg', \n","        classes=None\n","    )\n","\n","    self.lastL1   = tf.keras.layers.BatchNormalization()\n","    self.lastL2   = tf.keras.layers.Dropout(0.4)\n","    self.lastL3   = tf.keras.layers.Dense(n_classes, activation='softmax')\n","\n","\n","  def call(self, x):\n","\n","    x = self.mobilenet(x)\n","    x = self.lastL1(x)\n","    x = self.lastL2(x)\n","    x = self.lastL3(x)\n","\n","    return x\n","\n","\n","\n","#*********************************************************************************************************************************************\n","net_callbacks = [\n","\tPlotLosses(),\n","  tf.keras.callbacks.ModelCheckpoint(weights_path +  model + \"_\" +  dataset + \"{epoch:02d}\"  + '.h5',  monitor='val_acc', verbose=1, period=5, save_best_only=True, mode='max'),\n","\t#tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, min_delta=1, verbose=1, mode='auto', restore_best_weights=True)\n","]\n","print(\"net_callbacks var created\")\n","#*********************************************************************************************************************************************\n","\n","lr =  0.0001\n","\n","\n","if model == 'resnet':\n","  resnet = ResNet(n_classes=n_classes)\n","  resnet.compile(tf.keras.optimizers.Adam(lr), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=['acc'])\n","  resnet.build((batch_size, input_size[0], input_size[1], input_size[2]))\n","\n","elif model == 'mobilenet':\n","  mobilenet = MobileNet(n_classes=n_classes)\n","  mobilenet.compile(tf.keras.optimizers.Adam(lr), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=['acc'])\n","  mobilenet.build((batch_size, input_size[0], input_size[1], input_size[2]))\n","\n","elif model == 'cnn':\n","  simple_net = SimpleNet(n_classes=n_classes)\n","  simple_net.compile(tf.keras.optimizers.Adam(lr), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=['acc'])\n","  simple_net.build((batch_size, input_size[0], input_size[1], input_size[2]))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CWLwDkSV4gu9","colab_type":"code","colab":{}},"source":["#***********************************NETWORK*************************************\n","\n","\n","if model == 'resnet':\n","  res_history = resnet.fit(\n","      train_gen,\n","      epochs=n_epoch, \n","      verbose=2, \n","      callbacks=net_callbacks,  \n","      validation_data=valid_gen,\n","  )\n","\n","elif model == 'mobilenet':\n","  mobile_history = mobilenet.fit(\n","      train_gen,\n","      epochs=n_epoch, \n","      verbose=2, \n","      callbacks=net_callbacks,  \n","      validation_data=valid_gen,\n","  )\n","\n","elif model == 'cnn':\n","  simple_history = simple_net.fit(\n","      train_gen,\n","      epochs=n_epoch, \n","      verbose=2, \n","      callbacks=net_callbacks,  \n","      validation_data=valid_gen, \n","  )\n","\n","#*******************************************************************************"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PGES8LUqSUmr","colab_type":"code","colab":{}},"source":["\n","''' test set must be preprocessed like the training and valid generator data '''\n","if np.max(x_test) > 1:\n","    x_test = data_preprocessing(x_test)\n","\n","if model == \"resnet\":\n","  resnet.load_weights(weights_path + 'resnet_synth_quickdraw10.h5')\n","\n","  print(resnet.evaluate(x_test, y_test))\n","  embedding_net = resnet.resnet\n","\n","elif model == \"mobilenet\":\n","  mobilenet.load_weights(weights_path + 'mobilenet_quickdraw20.h5')    \n","\n","  print(mobilenet.evaluate(x_test, y_test))\n","  embedding_net = mobilenet.mobilenet\n","\n","\n","elif model == \"cnn\":\n","  simple_net.load_weights(weights_path + 'cnn_256_quickdraw15.h5')    \n","\n","  print(simple_net.evaluate(x_test, y_test))\n","  embedding_net = simple_net.embedding_net\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-o1Z78CdTO24","colab_type":"code","colab":{}},"source":["if model == \"cnn\":\n","  X_icons_eval = load_data(path=data_path + \"/icons_eval.pickle\", size=input_size[0], invert=False, _3d=False, randomize=False, rand_seed=random_seed)\n","else:\n","  X_icons_eval = load_data(path=data_path + \"/icons_eval.pickle\", size=input_size[0], invert=False, _3d=True, randomize=False, rand_seed=random_seed)\n","\n","# delete white images\n","X_icons_eval = np.asarray([i for i in X_icons_eval if np.min(i) != np.max(i)])\n","\n","X_icons_eval_edges = data_preprocessing(X_icons_eval)\n","\n","for _i, i in enumerate(X_icons_eval_edges): X_icons_eval_edges[_i] = contour_img(i)\n","\n","for i, im in enumerate(X_icons_eval_edges):\n","  if i < 10:\n","    show_img(im)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UaFtM0f_V-EZ","colab_type":"code","colab":{}},"source":["manual_eval = RankImages(X_icons_eval_edges, embedding_net, _n=15, _show_im=False, _show_dist=False)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dJ8AZlVSXdGm","colab_type":"code","colab":{}},"source":["\n","#target_im = paint_brush_img(w=input_size[0], h=input_size[1], line_width=30, preprocessed=True, show=False)\n","images = [cv2.imread(os.path.join(data_path, 'targets', im_path), 0) for im_path in os.listdir(os.path.join(data_path, 'targets')) if im_path.endswith('.jpg')]\n","images = [cv2.resize(i, (input_size[0], input_size[1])) for i in images]\n","\n","for image in images:\n","  target_im = image\n","\n","  if model == 'cnn': \n","      target_im = np.expand_dims(data_preprocessing(target_im), axis=-1)\n","  else:\n","      target_im = cv2.cvtColor(target_im, cv2.COLOR_GRAY2RGB)\n","\n","  # a or b\n","  #target_im_edges = data_preprocessing(target_im)  # a\n","  target_im_edges = contour_img(target_im)          # b\n","\n","  res = manual_eval.get_n_most_similar_images(target_im_edges, _returnType='indexes')\n","  manual_eval.format_result(target_im, [X_icons_eval_edges[r] for r in res])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2A49PlXcuy4_","colab_type":"code","colab":{}},"source":["target_im = paint_brush_img(w=input_size[0], h=input_size[1], line_width=30, preprocessed=True, show=False)\n","\n","if model == 'resnet': \n","  target_im = cv2.cvtColor(target_im, cv2.COLOR_GRAY2RGB)\n","\n","target_im_edges = contour_img(target_im)\n","\n","res = manual_eval.get_n_most_similar_images(target_im_edges, _returnType='indexes')\n","manual_eval.format_result(target_im, [X_icons_eval[r] for r in res])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JN4sdK1s16Im","colab_type":"code","colab":{}},"source":["\n","#***********************************FROM PYTHON MODEL TO JS MODEL*************************************\n","\n","#!pip install tensorflowjs\n","\n","# PRETRAINING MOBILENET_V1 MODEL\n","#mobilenet = MobileNet(n_classes=345)\n","#mobilenet.compile(tf.keras.optimizers.Adam(lr), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=['acc'])\n","#mobilenet.build((batch_size, input_size[0], input_size[1], input_size[2]))\n","#mobilenet.load_weights(weights_path + \"official/mobilenet_quickdraw_v1_20.h5\")\n","\n","#mobilenet.mobilenet.save(weights_path + \"models/mobilenet_quickdraw_v1_20.h5\")\n","\n","\n","# FINETUNED MOBILENET_V1 MODEL (MIXED MODE)\n","\n","#mobilenet = MobileNet(n_classes=169)\n","#mobilenet.compile(tf.keras.optimizers.Adam(lr), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=['acc'])\n","#mobilenet.build((batch_size, input_size[0], input_size[1], input_size[2]))\n","#mobilenet.load_weights(weights_path + \"official/mixed_mobilenet_mobilenet_quickdraw_v1_10.h5\")\n","\n","#mobilenet.mobilenet.save(weights_path + \"models/mixed_mobilenet_mobilenet_quickdraw_v1_10.h5\")\n","\n","\n","#!tensorflowjs_converter --input_format keras \"./models/mixed_mobilenet_mobilenet_quickdraw_v1_10.h5\" \"./models/mixed_mobilenet_mobilenet_quickdraw_v1_10_js.h5\""],"execution_count":0,"outputs":[]}]}